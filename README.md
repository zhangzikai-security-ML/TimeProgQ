# TimeProgQ
Title：TimeProgQ：Timestep-Aware Progressive Quantization for Balanced Speed-Quality in Diffusion Models

Abstract: Diffusion models have achieved remarkable success in generative tasks, but their heavy computational demands hinder practical deployment. Quantization is a promising solution to improve efficiency for diffusion models, but existing methods fail to balance training speed and generative quality.  In this paper, we identify the direct cause of low-quality and slow-speed, i.e., quality gradient instability and timestep quantization output error diversity. To this end, we propose TimeProgQ, a timestep-aware progressive quantization framework tailored for diffusion models. Our key innovations are threefold: (1) We enhance diffusion models with incremental quantization capability, enabling adaptive parameter handling that aligns with their iterative nature. (2) Leveraging this capability, we partition model parameters into unstructured groups and apply progressive multi-step quantization, which mitigates gradient instability by spreading the quantization impact across training stages. (3) We introduce a quantize-quality-aware loss control mechanism that dynamically adjusts loss weights based on denoising timesteps, accelerating training convergence without sacrificing quality. Extensive experiments demonstrate that TimeProgQ outperforms state-of-the-art quantization methods for diffusion models.
